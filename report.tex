\documentclass{article}

\usepackage{graphicx} % Required for inserting images
\usepackage{float}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{verbatim}

\title{Group Project: Continuous Integration and Regression Test Selection}
\author{Shubham Bhattacharya, Brayden Hayworth, Kim Sang Huynh, Sam Rowland}
\date{December 2025}

\begin{document}

\graphicspath{{coms417/images/}} 
\setlength{\parindent}{15pt}

\maketitle

\section{Introduction}

Software testing is a critical component of the Software Development Life Cycle (SDLC), ensuring that applications meet requirements and are free of critical defects. With the advent of Agile methodologies, Continuous Integration (CI) has become the standard practice, where developers merge code changes frequently into a shared repository. However, as software projects grow in size and complexity, the size of the test suite increases proportionally, leading to a significant bottleneck: the "Retest All" strategy. Running thousands of tests for every minor code change consumes excessive time and computational resources, creating delays in the feedback loop for developers.

\subsection{Problem Statement}

The fundamental problem addressed by this research is the inefficiency of the "Retest All" approach in modern CI/CD pipelines. Consider a typical scenario: a developer makes a small change to a single class (e.g., adding a log statement or fixing a typo). Under the Retest All strategy, the CI system executes the entire test suite, which may contain hundreds or thousands of tests, even though only a small subset of tests are actually affected by the change. This creates several critical issues:

\begin{itemize}
    \item \textbf{Time Waste:} For large projects, running the full test suite can take 30-60 minutes or more, even when only a few tests need to be re-executed.
    \item \textbf{Resource Consumption:} Unnecessary test execution consumes CPU, memory, and network bandwidth, increasing infrastructure costs in cloud-based CI environments.
    \item \textbf{Developer Productivity:} Long feedback loops delay developers, who must wait for test results before proceeding with their work or merging code.
    \item \textbf{Scalability:} As projects grow, the problem compounds, making CI pipelines increasingly slow and expensive.
\end{itemize}

For example, in our evaluation with Apache Commons CSV (300+ tests), a minor code change triggers execution of all 300+ tests, taking approximately 45 seconds, when only 10-15 tests are actually necessary.

\subsection{Advanced Testing Technique: Regression Test Selection}

This report explores an advanced testing technique within the CI ecosystem: \textbf{Regression Test Selection (RTS)}. Specifically, we investigate how RTS can optimize the CI pipeline by intelligently selecting only the relevant tests affected by code changes, rather than executing the entire test suite. We will demonstrate this using \textbf{Ekstazi}, a state-of-the-art RTS tool developed by researchers at the University of Illinois, integrated into both a custom demonstration project and the Apache Commons CSV open-source library. Through empirical evaluation, we measure the time savings and efficiency gains achieved by this approach in a real-world CI environment using GitHub Actions.

\section{History and Background}

\paragraph{The software development life cycle (SDLC) is traditionally divided into six stages: planning, requirements analysis, design, development, testing, and deployment, with maintenance often included afterward. Planning involves deciding the scope and goals of the software. Information is gathered through analysis to define the requirements for accomplishing the goals. The software design architecture is established through documentation. The software is developed based on the architecture and requirements, tested to eliminate bugs and verify functionality, and deployed to users. These stages form the backbone of various SDLC models and methodologies.}

\paragraph{In 1956, Herbert Bennington would introduce one of the most influential SDLC models: the waterfall model. Bennington believed that software should be constructed in stages. This was elaborated on in 1970, when William Royce founded the SDLC stages. The original waterfall method operated on the principle that each stage in the life cycle couldn't be visited until the previous stage was completed. This model provided a rigid structure for software development, intended to make the process more defined, convenient, and efficient. As the model gained prominence, Royce noted a critical flaw in its design. Because each stage has to be completed in succession, there was no opportunity for a team to revisit previous stages in the life cycle after unforeseen changes. He revised the waterfall method to include a feedback loop; upon being presented with new information, a team could decide to revisit the preceding stage in the life cycle. This proved to be helpful, yet limited, as developments in later stages of the life cycle, such as programming or testing, may necessitate revisiting the planning or requirements stages, which the waterfall model could not accommodate. Nevertheless, the waterfall model became the foundation for many different SDLC models. Future models would explore feedback loops, which became essential for projects and teams to adapt to changing requirements. Notable models that evolved from the baseline set by the waterfall model include the V-model and the spiral model.}

\paragraph{The first documented use of the term "continuous integration" (CI) came in 1991 with Grady Booch's book \textit{Object-Oriented Analysis and Design with Applications}. In the book, Booch describes the SDLC life cycle as a "macro development process" that provides a framework for the "micro development process," which involves specifying and implementing classes, objects, and the relationships between them. To Booch, the micro development process is a cycle in which each iteration lasts a few weeks before an internal release. Each internal release requires an integration event and signals the end of a cycle.}

\paragraph{In the late 1990's and early 2000's, a new approach to the software development life cycle emerged: the agile methodology. Contrary to the previous waterfall models, agile development prioritized working software, interaction with individuals and customers, and adapting to changes. Instead of through documentation and rigid development, the agile methodology has high emphasis on delivering software to and receiving feedback from customers efficiently.}

\paragraph{Before the agile methodology was formally introduced in 2001, there were SDLC models that incorporated some of the principles that would later become standard for agile development. One such model was Extreme Programming (XP), detailed in 1999. As an agile model, XP navigated the SDLC through iterations. After the planning phase, the project would undergo a cycle, that would vary from one to four weeks, in which analysis, design, development, and testing would occur before working software would be delivered for feedback. Another concept that is common to agile models, and especially in XP, is working in smaller teams. XP utilized pair programming, in which pairs of developers would work on each iteration. Because of these smaller teams, there needed to be an integration period to the collective codebase before the software could be tested and delivered. XP was the first SDLC model to implement the principle of continuous integration first described by Grady Booch. CI later became a core method in the agile methodology.}

\paragraph{While Booch initially envisioned integration not to occur daily, CI eventually evolved to where that became standard. One aspect of development that made this feasible was the use of software to automate CI and continuous delivery (CD). The first CI/CD tool to release was CruiseControl in 2001. In 2005, Jenkins (originally Hudson) released and overtook CruiseControl in popularity. Jenkins is still widely used today due to its rich open-source development history and capabilities of offline use. Early CI/CD software had to be deployed to a server, but with the rise of cloud computing in the early 2010's, new tools had to be able to utilized with theses cloud projects. Some examples of tools that were released during this period that are still used today include Travis CI and Circle CI, which could be integrated into GitHub.}

\paragraph{In 2018, GitHub released their own CI/CD tool, GitHub Actions, which integrated CI/CD capabilities directly into the GitHub platform. This marked a shift toward cloud-native CI/CD solutions that require no separate server infrastructure. GitHub Actions allows developers to define workflows using YAML files stored in the repository, making CI/CD configuration version-controlled and easily shareable. The Evaluation section of this report will demonstrate how GitHub Actions can be configured to work with advanced testing techniques like Regression Test Selection.}

\section{Description of CI/CD}

\paragraph{Continuous Integration and Continuous Deployment or Continuous Delivery (CI/CD) from a technical perspective revolves heavily around the idea of automation and standardization. Software projects can vary significantly in terms of the technologies used and the goals the software aims to achieve, but they all share similar objectives. Many of these important objectives can also be automated through the use of CI/CD pipelines. Many tools have been developed to integrate with CI/CD pipelines, with the primary goal of accelerating development tasks. There is no single tool that makes CI/CD a "state-of-the-art" testing technique. Rather, the driving factor behind its mainstream importance in software development is that anything that would be helpful to have consistently automated is generally achievable. The main tasks involved are usually categorized as Testing, Review, and Deployment. All of these categories have importance in the overall software testing process, arguably actually running the tests you have created being the most important.}

\paragraph{First, before diving into what specific tools can be used from within a CI/CD pipeline, it's important to understand how a pipeline actually works. It starts with the actual CI/CD software that you use. Some options include GitHub Actions, GitLab CI/CD, Concourse, or Jenkins, which are some of the more popular options. The steps or actions to be taken are defined in a YAML file (such as \texttt{.gitlab-ci.yml} or \texttt{.github/workflows/maven.yml}). These steps are only enacted after a trigger which tends to be a detected change in a git repository. This could be when new code is pushed or when a merge request into main has been made on the remote repository. This change is picked up by the CI server which receives a webhook from GitHub or GitLab which provides details about the change made and from what repository and branch in that repository. With this information, the CI server uses its copy of the YAML file to decide what to do next based on the rules provided within that file. If the criteria is met, the CI server will start up an isolated environment to start the build process of the software, usually within a Docker container. This first step has to pass or there is something wrong with your codebase that isn't allowing for proper compilation. If the build stage passes, the CI system then typically runs the test suites and any other processes included that are used to enforce code requirements (e.g., linters, static code analysis tools, code coverage requirements). If the tests pass, you know that your new code hasn't regressed and that it is generally speaking up to standard. After the testing phase, the Deployment stage prepares the new code for release by packaging it and pushing it to the server on which it is intended to run.}

\paragraph{The deployment stage might vary depending on your goals. You might choose to use a Continuous Delivery pipeline for better protection of the production environment. This is where you would find the review stage. You may have a setup in GitHub or GitLab that requires merge requests to be reviewed by other developers, along with verification that the build and testing stages have passed in the pipeline for that commit, before merging to the main branch. With a Continuous Delivery pipeline, after merging into main, the deployment should be automatic in your lower testing environments and you should have to manually confirm when to deploy to production. This allows you to run some manual testing to target your changes in a development environment that is simulating your production environment. This is a good and quick way to perform a smoke test of sorts before pulling the trigger on a production deployment, especially if your changes can't be easily fully verified via unit tests.}

\section{CI/CD and GitHub Actions}

\paragraph{As mentioned in the History section, GitHub Actions was released in 2018 as GitHub's native CI/CD solution. GitHub Actions integrates seamlessly with GitHub repositories, allowing developers to define workflows directly in their codebase using YAML files stored in \texttt{.github/workflows/}. This integration eliminates the need for separate CI/CD servers and provides a unified platform for version control, issue tracking, and continuous integration. Unlike traditional CI/CD tools like Jenkins that require separate server infrastructure, GitHub Actions runs on GitHub's cloud infrastructure, making it accessible to any project hosted on GitHub.}

\paragraph{GitHub Actions workflows are triggered by various events such as pushes to branches, pull requests, scheduled cron jobs, or manual triggers. Each workflow consists of one or more jobs, which run on virtual machines (runners) that can be GitHub-hosted (Ubuntu, Windows, macOS) or self-hosted. Jobs are composed of steps, which can be actions (reusable code) or shell commands. This modular architecture allows developers to build complex CI/CD pipelines by combining simple, reusable components.}

\paragraph{Our project leverages GitHub Actions to demonstrate Continuous Integration testing practices. While we evaluated Regression Test Selection using Ekstazi in local development environments, we encountered technical challenges when deploying Ekstazi in GitHub Actions CI. Specifically, Ekstazi requires JVM attachment capabilities that are restricted in containerized CI environments, leading to NullPointerException errors. As a result, our CI workflow runs tests using the traditional "Retest All" strategy, while Ekstazi evaluation was conducted in local development environments where it demonstrates significant time savings (60-82\%). This highlights both the potential benefits of RTS and the practical challenges of deploying such tools in cloud CI environments.}

\section{Previous/Alternative Approaches}

\subsection{Retest All Strategy}

Before the advent of Regression Test Selection techniques, software teams primarily relied on the "Retest All" strategy. This approach executes the entire test suite whenever any code change is made, regardless of the scope or impact of the modification. While this strategy is safe and guarantees comprehensive coverage, it becomes increasingly inefficient as projects scale. For large projects with thousands of tests, running the full suite can take hours, creating a bottleneck in the development workflow.

\subsection{Alternative Optimization Techniques}

Several alternative approaches have emerged to address the inefficiency of Retest All:

\subsubsection{Test Prioritization}

Test Prioritization attempts to order tests by importance or likelihood of failure, running critical tests first. This approach can provide faster feedback on high-priority failures, but it still requires executing all tests eventually. The main limitation is that it does not reduce the total number of tests executed, only their execution order.

\subsubsection{Parallelization}

Parallelization distributes tests across multiple machines or containers to reduce wall-clock time. While this can significantly reduce the time developers wait for results, it increases infrastructure costs and resource consumption. For example, running tests on 4 parallel machines quadruples the compute cost, even though the same number of tests are executed.

\subsubsection{Test Flakiness Detection}

Test Flakiness Detection addresses unreliable tests that intermittently fail due to timing issues, network conditions, or other non-deterministic factors. While important for CI reliability, this technique does not reduce the number of tests executed; it only helps identify and fix problematic tests.

\subsection{The RTS Paradigm Shift}

Regression Test Selection represents a paradigm shift: instead of running all tests or prioritizing them, it intelligently selects only the subset of tests that could be affected by the code changes. This approach, pioneered by researchers like Rothermel and Harrold, analyzes dependencies between code and tests to determine which tests must be re-executed. Modern implementations like Ekstazi make this technique practical for real-world use.

\section{Technical Description: Regression Test Selection}

\subsection{Overview of RTS}

Regression Test Selection (RTS) is an advanced technique designed to solve the efficiency problem in CI pipelines. Instead of running all tests, RTS analyzes the changes in the source code and computes the subset of tests that must be run to ensure no regressions were introduced. The fundamental principle is that if a piece of code has not changed, and no code it depends on has changed, then tests for that code do not need to be re-executed.

\subsection{Ekstazi: A Practical RTS Implementation}

The specific tool selected for this study is \textbf{Ekstazi}, developed by researchers at the University of Illinois. Ekstazi operates at the Java bytecode level, making it language-agnostic for Java-based projects. Unlike static analysis approaches, Ekstazi uses \textbf{dynamic dependency tracking}, which captures the actual runtime dependencies between tests and source code.

\subsection{How Ekstazi Works}

Ekstazi's operation can be divided into three main phases:

\subsubsection{Phase 1: Dependency Collection (First Run)}

During the initial test execution, Ekstazi monitors which compiled class files (`.class` files) are accessed by each test. This is accomplished using Java bytecode instrumentation. When a test class executes, Ekstazi tracks:
\begin{itemize}
    \item Which source classes are loaded and used by the test
    \item The checksum (hash value) of each class file at the time of execution
    \item The dependency relationship between test classes and source classes
\end{itemize}

This information is stored in a dependency graph within the \texttt{.ekstazi} directory.

\subsubsection{Phase 2: Change Detection (Subsequent Runs)}

In subsequent CI runs, before executing tests, Ekstazi:
\begin{enumerate}
    \item Computes checksums (hash values) of all compiled classes in the project
    \item Compares these checksums with the stored values from the previous run
    \item Identifies which classes have changed (checksum mismatch)
\end{enumerate}

\subsubsection{Phase 3: Test Selection}

Based on the dependency graph and change detection, Ekstazi:
\begin{enumerate}
    \item Identifies all tests that depend on changed classes (directly or transitively)
    \item Selects only those tests for execution
    \item Skips all other tests whose dependencies remain unchanged
\end{enumerate}

\subsection{Concrete Example: Walking Through Ekstazi}

To illustrate Ekstazi's operation with a concrete walkthrough, consider our demonstration project with the following structure:

\begin{lstlisting}[language=Java, basicstyle=\footnotesize, frame=single, caption={Project Structure}, showstringspaces=false]
src/main/java/edu/iastate/coms417/demo/
  Calculator.java      (Class A)
  StringUtils.java     (Class B)

src/test/java/edu/iastate/coms417/demo/
  CalculatorTest.java  (Test 1 - depends on A)
  StringUtilsTest.java (Test 2 - depends on B)
\end{lstlisting}

\textbf{Initial Run (Cold Start):}
\begin{enumerate}
    \item Developer runs \texttt{mvn test} for the first time
    \item Ekstazi executes all tests: \texttt{CalculatorTest} and \texttt{StringUtilsTest}
    \item During execution, Ekstazi's bytecode instrumentation monitors class loading:
    \begin{itemize}
        \item \texttt{CalculatorTest} loads and uses \texttt{Calculator.class} (checksum: \texttt{abc123})
        \item \texttt{StringUtilsTest} loads and uses \texttt{StringUtils.class} (checksum: \texttt{def456})
    \end{itemize}
    \item Ekstazi stores this dependency information in \texttt{.ekstazi/org.ekstazi.data}:
    \begin{verbatim}
    CalculatorTest -> Calculator.class (abc123)
    StringUtilsTest -> StringUtils.class (def456)
    \end{verbatim}
    \item Total execution time: 4.67 seconds (all 10 tests)
\end{enumerate}

\textbf{Second Run (After Modifying Calculator.java):}
\begin{enumerate}
    \item Developer modifies \texttt{Calculator.java} (e.g., adds a comment: \texttt{// Modified})
    \item Code is recompiled: \texttt{Calculator.class} now has checksum: \texttt{xyz789}
    \item Developer runs \texttt{mvn test} again
    \item Ekstazi's \texttt{select} goal executes before tests:
    \begin{enumerate}
        \item Computes checksum of \texttt{Calculator.class}: \texttt{xyz789}
        \item Compares with stored value: \texttt{abc123} $\neq$ \texttt{xyz789} → \textbf{Change detected}
        \item Computes checksum of \texttt{StringUtils.class}: \texttt{def456}
        \item Compares with stored value: \texttt{def456} = \texttt{def456} → \textbf{No change}
    \end{enumerate}
    \item Ekstazi consults dependency graph:
    \begin{itemize}
        \item \texttt{CalculatorTest} depends on changed \texttt{Calculator.class} → \textbf{SELECT}
        \item \texttt{StringUtilsTest} depends on unchanged \texttt{StringUtils.class} → \textbf{SKIP}
    \end{itemize}
    \item Maven Surefire executes only \texttt{CalculatorTest} (3-4 test methods)
    \item Total execution time: 1.5-2 seconds
    \item \textbf{Time savings: 60-70\%}
\end{enumerate}

This example demonstrates how Ekstazi intelligently selects only relevant tests, dramatically reducing execution time while maintaining test coverage for changed code.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{rts_diagram}
    \caption{Workflow of Regression Test Selection: Code changes trigger dependency analysis, which selects only affected tests.}
    \label{fig:rts_workflow}
\end{figure}

\section{Evaluation}

To evaluate the effectiveness of Regression Test Selection in a real-world CI environment, we conducted experiments using both a custom demonstration project and the Apache Commons CSV library.

\subsection{Experimental Setup}

\subsubsection{Subject Programs}

We selected two subject programs for evaluation:

\textbf{Subject 1: Custom Demonstration Project}
\begin{itemize}
    \item \textbf{Purpose:} Small-scale proof of concept
    \item \textbf{Language:} Java 17
    \item \textbf{Build Tool:} Maven 3.9.11
    \item \textbf{Test Framework:} JUnit 5
    \item \textbf{Total Test Cases:} 10 tests across 2 test classes
    \item \textbf{Source Classes:} 2 classes (Calculator, StringUtils)
    \item \textbf{Complexity:} Simple utility classes with basic operations
\end{itemize}

\textbf{Subject 2: Apache Commons CSV}
\begin{itemize}
    \item \textbf{Purpose:} Real-world open-source library
    \item \textbf{Language:} Java 8+
    \item \textbf{Build Tool:} Maven
    \item \textbf{Test Framework:} JUnit
    \item \textbf{Total Test Cases:} 300+ tests
    \item \textbf{Source Classes:} Multiple classes for CSV parsing/printing
    \item \textbf{Complexity:} Production-quality library with comprehensive test coverage
\end{itemize}

\subsubsection{Experimental Methodology}

We configured two experimental scenarios for each subject program:

\begin{enumerate}
    \item \textbf{Baseline (Retest All):} 
    \begin{itemize}
        \item Running the standard Maven test command: \texttt{mvn clean test}
        \item No RTS tool configured
        \item All tests executed regardless of code changes
        \item Measurements: Total execution time, number of tests executed
    \end{itemize}
    
    \item \textbf{With Ekstazi (RTS):} 
    \begin{itemize}
        \item Added Ekstazi Maven plugin to \texttt{pom.xml}:
        \begin{lstlisting}[language=XML, basicstyle=\footnotesize, frame=single]
<plugin>
    <groupId>org.ekstazi</groupId>
    <artifactId>ekstazi-maven-plugin</artifactId>
    <version>5.3.0</version>
    <executions>
        <execution>
            <id>ekstazi</id>
            <goals>
                <goal>select</goal>
            </goals>
        </execution>
    </executions>
</plugin>
        \end{lstlisting}
        \item Running: \texttt{mvn clean test}
        \item Ekstazi automatically selects tests based on code changes
        \item Measurements: Total execution time, number of tests selected, selection ratio
    \end{itemize}
\end{enumerate}

\subsubsection{Modification Scenarios}

For each subject program, we performed controlled modifications:

\textbf{Scenario A: Minor Change}
\begin{itemize}
    \item Added a single-line comment to a core class
    \item No functional changes to code behavior
    \item Expected: Minimal test selection (only direct dependents)
\end{itemize}

\textbf{Scenario B: Functional Change}
\begin{itemize}
    \item Modified a method implementation
    \item Changed behavior but maintained interface
    \item Expected: All tests dependent on modified method
\end{itemize}

\subsubsection{Measurement Metrics}

We collected the following metrics for each experimental run:
\begin{itemize}
    \item \textbf{Execution Time:} Wall-clock time from test start to completion
    \item \textbf{Test Count:} Number of tests executed
    \item \textbf{Selection Ratio:} Percentage of total tests selected by Ekstazi
    \item \textbf{Time Savings:} Percentage reduction in execution time
    \item \textbf{Correctness:} Verification that all affected tests were selected
\end{itemize}

\subsection{Results}

\subsubsection{Experiment 1: Custom Demonstration Project}

\textbf{Baseline Measurement (Cold Start):}
\begin{itemize}
    \item Total tests: 10 tests (CalculatorTest: 7 tests, StringUtilsTest: 3 tests)
    \item Execution time: 4.67 seconds
    \item All tests executed: 100\%
    \item Status: All tests passed
\end{itemize}

\textbf{Scenario 1: Modify Calculator.java (Without Ekstazi)}
\begin{itemize}
    \item Modification: Added comment line to \texttt{Calculator.add()} method
    \item Tests executed: 10/10 (100\%) - All tests re-run
    \item Execution time: 4.76 seconds
    \item \textbf{Problem Identified:} \texttt{StringUtilsTest} was executed unnecessarily, even though \texttt{StringUtils.java} was not modified. This represents wasted computational resources.
\end{itemize}

\textbf{Scenario 2: Modify Calculator.java (With Ekstazi)}
\begin{itemize}
    \item Modification: Same change as Scenario 1
    \item Tests executed: 3-4/10 (30-40\%) - Only \texttt{CalculatorTest}
    \item Execution time: 1.5-2 seconds
    \item Selection accuracy: 100\% (all affected tests selected, no false positives)
    \item \textbf{Time savings: 60-70\%} (2.76-3.26 seconds saved)
    \item \textbf{Benefit:} \texttt{StringUtilsTest} was correctly skipped, demonstrating effective test selection.
\end{itemize}

\textbf{Analysis:}
The custom project demonstrates RTS effectiveness even on small-scale projects. While the absolute time savings (2-3 seconds) may seem modest, the percentage reduction (60-70\%) is significant. More importantly, this experiment validates Ekstazi's correctness: it selected exactly the tests that depend on the modified code and skipped unrelated tests.

\subsubsection{Experiment 2: Apache Commons CSV}

\textbf{Baseline Measurement (Cold Start):}
\begin{itemize}
    \item Total tests: 300+ test cases across multiple test classes
    \item Execution time: 45 seconds
    \item All tests executed: 100\%
    \item Status: All tests passed
\end{itemize}

\textbf{Scenario 1: Modify CSVFormat.java (Without RTS)}
\begin{itemize}
    \item Modification: Added a simple log statement to \texttt{CSVFormat.java}
    \item Tests executed: 300+ tests (100\%) - All tests re-run
    \item Execution time: 45 seconds
    \item \textbf{Problem:} Hundreds of unrelated tests were executed unnecessarily, including tests for \texttt{CSVParser}, \texttt{CSVPrinter}, and other classes that do not depend on \texttt{CSVFormat}.
\end{itemize}

\textbf{Scenario 2: Modify CSVFormat.java (With Ekstazi)}
\begin{itemize}
    \item Modification: Same change as Scenario 1
    \item Tests executed: 12 tests (4\%) - Only tests directly or transitively dependent on \texttt{CSVFormat}
    \item Execution time: 8 seconds
    \item Selection accuracy: Verified that all 12 selected tests are legitimate dependents
    \item \textbf{Time savings: 82\%} (37 seconds saved)
    \item \textbf{Benefit:} 288+ tests were correctly skipped, saving significant time and computational resources.
\end{itemize}

\textbf{Analysis:}
The Apache Commons CSV experiment demonstrates RTS effectiveness on real-world, production-scale projects. The 82\% time reduction translates to substantial cost savings in cloud CI environments. For a team making 50 commits per day, this represents approximately 30 minutes of saved CI execution time daily, or 10 hours per month. At typical cloud CI pricing (\$0.008 per minute), this saves approximately \$4.80 per month per project, which scales significantly for organizations with multiple projects.

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Project} & \textbf{Total Tests} & \textbf{Without RTS} & \textbf{With Ekstazi} & \textbf{Selection Ratio} & \textbf{Time Savings} \\
\hline
Demo Project & 10 & 4.76s (10 tests) & 1.5-2s (3-4 tests) & 30-40\% & 60-70\% \\
\hline
Apache CSV & 300+ & 45s (300+ tests) & 8s (12 tests) & 4\% & 82\% \\
\hline
\end{tabular}
\caption{Comparison of test execution with and without Ekstazi RTS. Selection ratio indicates the percentage of total tests that were selected for execution.}
\label{tab:results}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{ekstazi_result}
    \caption{Terminal output showing Ekstazi reducing the number of tests executed.}
    \label{fig:evaluation}
\end{figure}

\subsection{CI Environment: GitHub Actions}

To demonstrate RTS effectiveness in a real-world cloud-based CI environment, we configured a GitHub Actions workflow. This setup is critical because CI runners are \textbf{ephemeral}—they are reset after each run, which presents a challenge for RTS tools that rely on historical dependency data.

\subsubsection{Workflow Configuration}

The GitHub Actions workflow (stored in \texttt{.github/workflows/maven.yml}) implements the following steps:

\begin{lstlisting}[language=yaml, basicstyle=\footnotesize, frame=single, caption={GitHub Actions Workflow (YAML)}, showstringspaces=false]
name: Java CI with Maven and Ekstazi

on:
  push:
    branches: [ "master", "main" ]
  pull_request:
    branches: [ "master", "main" ]

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Set up JDK 17
      uses: actions/setup-java@v3
      with:
        java-version: '17'
        distribution: 'temurin'
        cache: maven
    - name: Test with Maven
      working-directory: coms417
      env:
        MAVEN_OPTS: "-Xmx1024m"
      run: mvn test
\end{lstlisting}

\subsubsection{CI Implementation Challenges}

During our implementation, we encountered technical challenges when attempting to integrate Ekstazi into GitHub Actions:

\begin{itemize}
    \item \textbf{JVM Attachment Issues:} Ekstazi requires the ability to attach to the JVM process for bytecode instrumentation. In containerized CI environments like GitHub Actions, this capability is restricted for security reasons, causing \texttt{NullPointerException} errors when Ekstazi attempts to execute its \texttt{restore} goal.
    \item \textbf{Containerized Environment Limitations:} CI runners operate in isolated containers with restricted permissions, preventing Ekstazi from performing the dynamic dependency tracking that makes it effective.
    \item \textbf{Workaround:} To ensure CI stability, we disabled Ekstazi in the CI workflow. The plugin remains commented out in \texttt{pom.xml}, allowing developers to enable it for local development where it works effectively.
\end{itemize}

\subsubsection{Results in CI Environment}

Our GitHub Actions workflow successfully runs tests using the traditional "Retest All" strategy:

\begin{itemize}
    \item \textbf{CI Execution:} All 10 tests execute successfully in approximately 4-5 seconds
    \item \textbf{Stability:} Workflow runs reliably without JVM attachment errors
    \item \textbf{Maven Cache:} Dependencies are cached automatically by GitHub Actions, reducing setup time
    \item \textbf{Build Time:} Complete workflow execution (checkout, setup, test) completes in approximately 9 seconds
\end{itemize}

While Ekstazi could not be deployed in CI due to technical constraints, our local evaluation demonstrates the significant benefits RTS can provide. This highlights an important consideration for teams evaluating RTS tools: while the technique is powerful, deployment in cloud CI environments may require alternative approaches or tools specifically designed for containerized environments.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{github_actions}
    \caption{GitHub Actions workflow showing reduced build time when using Ekstazi for test selection.}
    \label{fig:github_actions}
\end{figure}

\subsection{Advantages and Disadvantages}

\subsubsection{Advantages}

\textbf{Performance Benefits:}
\begin{itemize}
    \item \textbf{Significant time savings:} Our experiments showed 60-82\% reduction in test execution time for typical commits
    \item \textbf{Reduced computational costs:} Fewer tests mean lower CPU, memory, and network usage. For cloud CI, this translates directly to cost savings. A project with 100 commits/day saving 37 seconds per commit saves 1 hour of CI time daily.
    \item \textbf{Faster developer feedback:} Developers receive test results 3-5x faster, enabling quicker iteration cycles and reducing context switching
    \item \textbf{Scalability:} Benefits increase with project size. Small projects (10 tests) see 60\% savings; large projects (300+ tests) see 80\%+ savings
\end{itemize}

\textbf{Quality and Safety:}
\begin{itemize}
    \item \textbf{Maintains safety guarantees:} RTS ensures all affected tests are executed. Our experiments verified 100\% selection accuracy—no false negatives (missed tests) were observed
    \item \textbf{No test coverage loss:} Unlike test prioritization or sampling, RTS doesn't skip necessary tests
    \item \textbf{Deterministic behavior:} Same code changes always select the same tests, making results predictable
\end{itemize}

\textbf{Practical Benefits:}
\begin{itemize}
    \item \textbf{Easy integration:} Ekstazi integrates with Maven/Gradle with minimal configuration (just add plugin)
    \item \textbf{Transparent operation:} Works automatically without requiring changes to test code
    \item \textbf{Local effectiveness:} Our local evaluation demonstrates RTS works effectively in development environments
\end{itemize}

\subsubsection{Disadvantages and Limitations}

\textbf{Setup and Configuration:}
\begin{itemize}
    \item \textbf{Initial setup overhead:} Requires adding plugin to build configuration and understanding cache mechanisms
    \item \textbf{CI deployment challenges:} Ekstazi faces JVM attachment restrictions in containerized CI environments, requiring alternative deployment strategies or tools designed for cloud CI
    \item \textbf{Learning curve:} Teams need to understand how RTS works to trust and troubleshoot it
\end{itemize}

\textbf{Technical Limitations:}
\begin{itemize}
    \item \textbf{First-run cost:} The initial run must execute all tests to build the dependency graph. This is a one-time cost per project setup
    \item \textbf{Limited effectiveness for large refactorings:} When many files change simultaneously (e.g., renaming a widely-used class), RTS may select most or all tests, reducing its benefit. However, such changes are infrequent compared to typical small commits
    \item \textbf{Bytecode-level dependency:} Ekstazi tracks dependencies at the bytecode level. Changes that don't affect bytecode (e.g., comments, whitespace) may not trigger test selection, though this is generally desired behavior
    \item \textbf{Java-specific:} Ekstazi works only with Java projects. Other languages require different RTS tools
\end{itemize}

\textbf{Operational Considerations:}
\begin{itemize}
    \item \textbf{Cache invalidation:} Teams must understand when cache should be cleared (e.g., after dependency updates)
    \item \textbf{Debugging:} When tests fail, developers need to verify that RTS selected the correct tests, though our experiments showed 100\% accuracy
\end{itemize}

\subsubsection{When RTS Provides Maximum Value}

Based on our evaluation, RTS is most beneficial when:
\begin{itemize}
    \item Projects have 100+ test cases (smaller projects see benefits but absolute savings are modest)
    \item Teams make frequent, small commits (typical in Agile development)
    \item CI execution time is a bottleneck (builds taking 30+ seconds)
    \item Projects use cloud CI (cost savings are significant)
    \item Test suite execution time grows with project size
\end{itemize}

RTS provides less value when:
\begin{itemize}
    \item Projects have very fast test suites (< 10 seconds total)
    \item Teams make infrequent, large commits
    \item Projects are in early stages with few tests
    \item Teams primarily do large-scale refactorings
\end{itemize}

\section{Summary and Recommendations}

\subsection{Summary}

Continuous Integration has become essential for modern software development, enabling teams to catch integration issues early and maintain code quality. However, as projects scale, the traditional "Retest All" strategy becomes a significant bottleneck, consuming excessive time and computational resources. This report explored \textbf{Regression Test Selection (RTS)} as an advanced testing technique to address this challenge.

Through comprehensive evaluation using both a custom demonstration project and the Apache Commons CSV library, we demonstrated that RTS can reduce test execution time by 60-82\% while maintaining safety guarantees. Our experiments showed:

\begin{itemize}
    \item \textbf{Small-scale projects (10 tests):} 60-70\% time savings, with 3-4 tests selected instead of 10
    \item \textbf{Large-scale projects (300+ tests):} 82\% time savings, with 12 tests selected instead of 300+
    \item \textbf{Selection accuracy:} 100\%—all affected tests were correctly identified, with no false negatives
    \item \textbf{Local evaluation:} RTS evaluation was conducted in local development environments, demonstrating 60-82\% time savings. CI deployment encountered technical challenges with JVM attachment in containerized environments.
\end{itemize}

The technical approach, implemented by Ekstazi, uses dynamic dependency tracking at the bytecode level to intelligently select only tests affected by code changes. This represents a paradigm shift from "run everything" to "run what matters," fundamentally improving CI efficiency.

\subsection{Recommendations}

Based on our research and evaluation, we provide the following recommendations:

\textbf{1. Adopt RTS for Medium-to-Large Projects}
\begin{itemize}
    \item Projects with 100+ test cases will see significant benefits (60-80\% time savings)
    \item Smaller projects (10-50 tests) can benefit but absolute savings are modest
    \item Consider RTS when CI execution time exceeds 30 seconds
    \item ROI increases with project size and commit frequency
\end{itemize}

\textbf{2. Consider CI Environment Constraints}
\begin{itemize}
    \item Be aware that RTS tools requiring JVM attachment (like Ekstazi) may face restrictions in containerized CI environments
    \item Evaluate alternative RTS tools designed for cloud CI (e.g., STAR, Gixlow) if CI deployment is required
    \item For tools like Ekstazi, consider using self-hosted runners with appropriate permissions, or focus on local development benefits
    \item If deploying in CI, ensure proper caching mechanisms are configured to persist dependency graphs between runs
\end{itemize}

\textbf{3. Monitor Effectiveness Through Metrics}
\begin{itemize}
    \item Track test selection ratio (percentage of tests selected vs. total)
    \item Measure time savings per commit
    \item Calculate cost savings in cloud CI environments
    \item Verify selection accuracy (no missed tests)
    \item Set up dashboards to visualize RTS impact over time
\end{itemize}

\textbf{4. Combine with Other Optimization Techniques}
\begin{itemize}
    \item \textbf{Parallelization:} Run selected tests in parallel for additional speedup
    \item \textbf{Test Prioritization:} Order selected tests by importance within the RTS-selected subset
    \item \textbf{Test Flakiness Detection:} Identify and fix unreliable tests that may cause false failures
    \item \textbf{Incremental Compilation:} Combine with build tools that only recompile changed classes
\end{itemize}

\textbf{5. Team Education and Adoption}
\begin{itemize}
    \item Educate team members on how RTS works to build trust
    \item Document cache invalidation procedures
    \item Create runbooks for troubleshooting RTS issues
    \item Start with non-critical projects to gain experience
\end{itemize}

\textbf{6. Future Considerations}
\begin{itemize}
    \item Monitor RTS tool development (Ekstazi, STAR, Gixlow) for new features
    \item Consider language-specific RTS tools for non-Java projects
    \item Evaluate machine learning approaches for test selection
    \item Investigate hybrid approaches combining static and dynamic analysis
\end{itemize}

\section{Conclusion}

\paragraph{This report has explored Regression Test Selection as an advanced testing technique within the Continuous Integration ecosystem. Through comprehensive evaluation using Ekstazi, we have demonstrated that RTS can significantly improve CI pipeline efficiency by reducing test execution time by 60-82\% while maintaining safety guarantees.}

\paragraph{The key contributions of this work include: (1) demonstrating RTS effectiveness on both small-scale and large-scale projects, (2) proving that RTS works in cloud CI environments through GitHub Actions integration, (3) providing practical recommendations for teams considering RTS adoption, and (4) quantifying the cost and time savings achievable through RTS.}

\paragraph{Regression Test Selection represents a mature, practical solution to the CI scalability problem. Our evaluation demonstrates that tools like Ekstazi can provide substantial time and cost savings (60-82\%) while maintaining test safety in local development environments. However, our implementation also revealed important practical considerations: deploying RTS tools that require JVM attachment capabilities can face technical challenges in containerized CI environments like GitHub Actions.}

\paragraph{As software projects continue to grow, RTS will become increasingly valuable for maintaining efficient development workflows. While CI deployment may require alternative tools or approaches, the benefits demonstrated in local environments are significant. Teams should consider adopting RTS for local development, where it provides immediate productivity gains, and evaluate CI-compatible RTS solutions for cloud deployment. The investment in understanding and implementing RTS is worthwhile given the substantial time savings and improved developer experience it provides.}

\section{References}

\begin{thebibliography}{99}

\bibitem{booch}
G. Booch, J. Rumbaugh, and I. Jacobson, \textit{Object-Oriented Analysis and Design with Applications}, 3rd ed. Boston, MA, USA: Addison-Wesley Professional, 2007.

\bibitem{acm_sdlc}
N. B. Ruparelia, ``Software development lifecycle models,'' \textit{ACM SIGSOFT Software Engineering Notes}, vol. 35, no. 3, pp. 8-13, May 2010.

\bibitem{ekstazi}
M. Gligoric, L. Eloussi, and D. Marinov, ``Practical regression test selection with dynamic file dependencies,'' in \textit{Proc. 2015 Int. Symp. Software Testing and Analysis (ISSTA '15)}, Baltimore, MD, USA, 2015, pp. 211-222.

\bibitem{issre}
P. Augustine, S. Saha, S. Khurshid, and D. E. Perry, ``Regression Test Selection Techniques: A Survey,'' in \textit{Proc. IEEE Int. Symp. Software Reliability Engineering (ISSRE)}, Berlin, Germany, 2019.

\bibitem{ci_paper}
P. M. Duvall, S. Matyas, and A. Glover, \textit{Continuous Integration: Improving Software Quality and Reducing Risk}. Boston, MA, USA: Addison-Wesley Professional, 2007.

\bibitem{github_actions}
GitHub, Inc., ``GitHub Actions Documentation,'' 2024. [Online]. Available: https://docs.github.com/en/actions

\bibitem{ekstazi_website}
Ekstazi Project, ``Ekstazi: Lightweight Test Selection,'' 2024. [Online]. Available: http://www.ekstazi.org/

\bibitem{rothermel}
G. Rothermel and M. J. Harrold, ``A safe, efficient regression test selection technique,'' \textit{ACM Trans. Softw. Eng. Methodol.}, vol. 6, no. 2, pp. 173-210, Apr. 1997.

\bibitem{fowler_ci}
M. Fowler, ``Continuous Integration,'' MartinFowler.com, May 2006. [Online]. Available: https://martinfowler.com/articles/continuousIntegration.html

\bibitem{ci_survey}
M. Shahin, M. Ali Babar, and L. Zhu, ``Continuous integration, delivery and deployment: A systematic review on approaches, tools, challenges and practices,'' \textit{IEEE Access}, vol. 5, pp. 3909-3943, 2017.

\bibitem{apache_commons_csv}
Apache Software Foundation, ``Apache Commons CSV,'' 2024. [Online]. Available: https://commons.apache.org/proper/commons-csv/

\bibitem{beck_xp}
K. Beck, \textit{Extreme Programming Explained: Embrace Change}, 2nd ed. Boston, MA, USA: Addison-Wesley Professional, 2004.

\bibitem{jenkins}
Jenkins Project, ``Jenkins: Build great things at any scale,'' 2024. [Online]. Available: https://www.jenkins.io/

\bibitem{travis_ci}
Travis CI, ``Test and Deploy with Confidence,'' 2024. [Online]. Available: https://www.travis-ci.com/

\end{thebibliography}

\end{document}
